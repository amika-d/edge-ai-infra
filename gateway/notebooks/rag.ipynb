{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6a32e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a376c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Data models\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class PageContent:\n",
    "    \"\"\"Raw content extracted from a single PDF page.\"\"\"\n",
    "    page_number: int          # 1-based\n",
    "    raw_text: str\n",
    "    headings: list[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ParsedDocument:\n",
    "    \"\"\"Full parsed representation of a PDF document.\"\"\"\n",
    "    document_id: str          # e.g. SHA-256 hash or a UUID\n",
    "    document_name: str        # original filename\n",
    "    file_path: str\n",
    "    total_pages: int\n",
    "    pages: list[PageContent] = field(default_factory=list)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Heading detection\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Patterns that suggest a line is a section heading in legal documents.\n",
    "_HEADING_PATTERNS = [\n",
    "    # ALL-CAPS lines (common in contracts: \"TERMINATION\", \"REPRESENTATIONS\")\n",
    "    re.compile(r\"^[A-Z][A-Z\\s\\-&,\\.]{4,}$\"),\n",
    "    # Numbered sections: \"1.\", \"1.1\", \"Section 2\", \"Article IV\"\n",
    "    re.compile(r\"^(?:Section|Article|Clause|Schedule|Exhibit|Annex)?\\s*\\d+(?:\\.\\d+)*[\\.\\)]\\s+\\S\", re.I),\n",
    "    # Roman numeral headings: \"IV. Obligations\"\n",
    "    re.compile(r\"^[IVXLCDM]+\\.\\s+\\S\", re.I),\n",
    "    # Title-case lines that are short (≤ 80 chars) and end without a period\n",
    "    re.compile(r\"^[A-Z][A-Za-z\\s]{3,79}[^.]$\"),\n",
    "]\n",
    "\n",
    "\n",
    "def _detect_headings(text: str) -> list[str]:\n",
    "    \"\"\"Return a list of lines from *text* that look like section headings.\"\"\"\n",
    "    headings = []\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or len(line) > 120:\n",
    "            continue\n",
    "        for pattern in _HEADING_PATTERNS:\n",
    "            if pattern.match(line):\n",
    "                headings.append(line)\n",
    "                break\n",
    "    return headings\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Core parser\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class DocumentParser:\n",
    "    \"\"\"\n",
    "    Parse a PDF file and return a :class:`ParsedDocument` with per-page\n",
    "    text and detected headings.\n",
    "\n",
    "    Usage::\n",
    "\n",
    "        parser = DocumentParser()\n",
    "        doc = parser.parse(\"contracts/Lease_Agreement_2024.pdf\")\n",
    "        for page in doc.pages:\n",
    "            print(page.page_number, page.headings)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, detect_headings: bool = True):\n",
    "        self.detect_headings = detect_headings\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Public API\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def parse(\n",
    "        self,\n",
    "        file_path: str | Path,\n",
    "        document_id: Optional[str] = None,\n",
    "    ) -> ParsedDocument:\n",
    "        \"\"\"\n",
    "        Parse *file_path* and return a fully-populated :class:`ParsedDocument`.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the PDF file.\n",
    "            document_id: Optional stable identifier. Defaults to the stem of\n",
    "                         the filename if not provided.\n",
    "\n",
    "        Returns:\n",
    "            ParsedDocument with one :class:`PageContent` entry per page.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the file does not exist.\n",
    "            ValueError: If the file is not a valid PDF.\n",
    "        \"\"\"\n",
    "        path = Path(file_path)\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"PDF not found: {path}\")\n",
    "\n",
    "        doc_name = path.name\n",
    "        doc_id = document_id or path.stem\n",
    "\n",
    "        pages: list[PageContent] = []\n",
    "\n",
    "        try:\n",
    "            pdf = fitz.open(str(path))\n",
    "        except Exception as exc:\n",
    "            raise ValueError(f\"Could not open PDF '{path}': {exc}\") from exc\n",
    "\n",
    "        with pdf:\n",
    "            total_pages = pdf.page_count\n",
    "            for page_index in range(total_pages):\n",
    "                page = pdf[page_index]\n",
    "                page_number = page_index + 1  # convert to 1-based\n",
    "\n",
    "                # Extract text – \"text\" mode gives plain UTF-8 output\n",
    "                raw_text = page.get_text(\"text\")\n",
    "                raw_text = self._clean_text(raw_text)\n",
    "\n",
    "                headings = (\n",
    "                    _detect_headings(raw_text) if self.detect_headings else []\n",
    "                )\n",
    "\n",
    "                pages.append(\n",
    "                    PageContent(\n",
    "                        page_number=page_number,\n",
    "                        raw_text=raw_text,\n",
    "                        headings=headings,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return ParsedDocument(\n",
    "            document_id=doc_id,\n",
    "            document_name=doc_name,\n",
    "            file_path=str(path.resolve()),\n",
    "            total_pages=total_pages,\n",
    "            pages=pages,\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Helpers\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean_text(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Light normalisation:\n",
    "          - Remove null bytes and form-feeds.\n",
    "          - Collapse runs of more than two blank lines.\n",
    "          - Strip trailing whitespace from each line.\n",
    "        \"\"\"\n",
    "        text = text.replace(\"\\x00\", \"\").replace(\"\\f\", \"\\n\")\n",
    "        text = \"\\n\".join(line.rstrip() for line in text.splitlines())\n",
    "        # Collapse 3+ consecutive blank lines into 2\n",
    "        text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "        return text.strip()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Convenience function\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def parse_pdf(\n",
    "    file_path: str | Path,\n",
    "    document_id: Optional[str] = None,\n",
    ") -> ParsedDocument:\n",
    "    \"\"\"\n",
    "    Module-level shortcut for one-off parsing.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        from services.rag.document_parser import parse_pdf\n",
    "\n",
    "\n",
    "        doc = parse_pdf(\"contracts/Lease_Agreement_2024.pdf\")\n",
    "        print(doc.total_pages)\n",
    "        print(doc.pages[0].headings)\n",
    "    \"\"\"\n",
    "    return DocumentParser().parse(file_path, document_id=document_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f79e2d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Dir: /home/bota/personal/edge_computing\n",
      "Data Path: /home/bota/personal/edge_computing/gateway/data\n",
      "Is Debug True? False\n",
      "VLLM API URL loaded? True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = Path(\"/home/bota/personal/edge_computing\")\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "from gateway.core.config import settings\n",
    "print(\"Base Dir:\", PROJECT_ROOT)\n",
    "print(\"Data Path:\", settings.DATA_PATH)\n",
    "print(\"Is Debug True?\", settings.DEBUG)\n",
    "print(\"VLLM API URL loaded?\", bool(settings.VLLM_API_URL))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ba868b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed file path: /home/bota/personal/edge_computing/gateway/data/docs/lease_agreement.pdf\n"
     ]
    }
   ],
   "source": [
    "file_path = Path(settings.DATA_PATH) / \"docs\" / \"lease_agreement.pdf\"\n",
    "print(\"Constructed file path:\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8d2e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_pdf = parse_pdf(file_path=file_path, document_id=\"lease_agreement_2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5484e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed document ID: lease_agreement_2024\n",
      "Document name: lease_agreement.pdf\n",
      "Total pages: 9\n",
      "Headings on first page: ['7.  LANDLORD’S OTHER RESPONSIBILITIES', '8.  TENANT’S OTHER RESPONSIBILITIES', 'Initials', 'Initials']\n"
     ]
    }
   ],
   "source": [
    "print(\"Parsed document ID:\", parse_pdf.document_id)\n",
    "print(\"Document name:\", parse_pdf.document_name)\n",
    "print(\"Total pages:\", parse_pdf.total_pages)\n",
    "print(\"Headings on first page:\", parse_pdf.pages[3].headings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e02d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# services/rag/agreement_chunker.py\n",
    "\n",
    "from __future__ import annotations\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.datamodel.base_models import InputFormat, ConversionStatus\n",
    "from docling.chunking import HybridChunker\n",
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration (Agreement Only)\n",
    "# -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class AgreementChunkConfig:\n",
    "    max_tokens: int = 722\n",
    "    overlap_tokens: int = 100\n",
    "    min_chunk_words: int = 30\n",
    "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    metadata: dict\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Agreement Chunker\n",
    "# -----------------------------\n",
    "\n",
    "class AgreementChunker:\n",
    "\n",
    "    def __init__(self, config: Optional[AgreementChunkConfig] = None):\n",
    "        self.config = config or AgreementChunkConfig()\n",
    "\n",
    "        self._converter = DocumentConverter(\n",
    "            format_options={\n",
    "                InputFormat.PDF: PdfFormatOption(\n",
    "                    pipeline_options=PdfPipelineOptions(\n",
    "                        do_ocr=False,\n",
    "                        do_table_structure=False,\n",
    "                        generate_page_images=False,\n",
    "                    )\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "        tokenizer = HuggingFaceTokenizer(\n",
    "            tokenizer=AutoTokenizer.from_pretrained(self.config.embedding_model),\n",
    "            max_tokens=self.config.max_tokens,\n",
    "        )\n",
    "\n",
    "        self._chunker = HybridChunker(\n",
    "            tokenizer=tokenizer,\n",
    "            merge_peers=True,\n",
    "        )\n",
    "\n",
    "    def chunk_pdf(self, path: str | Path) -> List[Chunk]:\n",
    "        path = str(path)\n",
    "        doc_name = Path(path).name\n",
    "        doc_id = Path(path).stem\n",
    "\n",
    "        result = self._converter.convert(path)\n",
    "\n",
    "        if result.status == ConversionStatus.FAILURE:\n",
    "            raise RuntimeError(\"PDF conversion failed\")\n",
    "\n",
    "        dl_doc = result.document\n",
    "        raw_chunks = list(self._chunker.chunk(dl_doc))\n",
    "\n",
    "        chunks = []\n",
    "        for idx, raw in enumerate(raw_chunks):\n",
    "            text = raw.text.strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # Basic page detection\n",
    "            page_number = None\n",
    "            doc_items = getattr(raw.meta, \"doc_items\", [])\n",
    "            if doc_items:\n",
    "                prov = getattr(doc_items[0], \"prov\", [])\n",
    "                if prov:\n",
    "                    page_number = getattr(prov[0], \"page_no\", None)\n",
    "\n",
    "            headings = getattr(raw.meta, \"headings\", []) or []\n",
    "            section_title = \" > \".join(headings) if headings else \"Unknown Section\"\n",
    "\n",
    "            if len(text.split()) < self.config.min_chunk_words:\n",
    "                continue\n",
    "\n",
    "            chunks.append(\n",
    "                Chunk(\n",
    "                    chunk_id=f\"{doc_id}_{uuid.uuid4().hex[:8]}\",\n",
    "                    text=text,\n",
    "                    metadata={\n",
    "                        \"document_id\": doc_id,\n",
    "                        \"document_name\": doc_name,\n",
    "                        \"page_number\": page_number,\n",
    "                        \"section_title\": section_title,\n",
    "                        \"chunk_index\": idx,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4dbacc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# services/rag/agreement_indexer.py\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# Load embedding model ONCE\n",
    "# ----------------------------------\n",
    "\n",
    "embedding_model = SentenceTransformer(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "dimension = embedding_model.get_sentence_embedding_dimension()\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "metadata_store = []\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# Ingest Agreement\n",
    "# ----------------------------------\n",
    "\n",
    "def ingest_agreement(pdf_path: str):\n",
    "\n",
    "    chunker = AgreementChunker()\n",
    "    chunks = chunker.chunk_pdf(pdf_path)\n",
    "\n",
    "    texts = [c.text for c in chunks]\n",
    "    embeddings = embedding_model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "    index.add(embeddings)\n",
    "    metadata_store.extend(chunks)\n",
    "\n",
    "    print(f\"Ingested {len(chunks)} chunks\\nembeddings shape: {embeddings.shape}\")\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# Retrieval\n",
    "# ----------------------------------\n",
    "\n",
    "def search(query: str, top_k: int = 5):\n",
    "\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        results.append(metadata_store[idx])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57a43df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 20 chunks from the agreement.\n",
      "Sample chunk metadata: {'document_id': 'lease_agreement', 'document_name': 'lease_agreement.pdf', 'page_number': 1, 'section_title': 'LEASE AGREEMENT - RESIDENTIAL', 'chunk_index': 0}\n"
     ]
    }
   ],
   "source": [
    "chunker = AgreementChunker()\n",
    "chunks = chunker.chunk_pdf(file_path)\n",
    "print(f\"Generated {len(chunks)} chunks from the agreement.\")\n",
    "print(\"Sample chunk metadata:\", chunks[0].metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b5f73d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 20 chunks\n",
      "embeddings shape: (20, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 20 chunks\n",
      "embeddings shape: (20, 384)\n",
      "ingest agreement: None\n"
     ]
    }
   ],
   "source": [
    "ingested = ingest_agreement(file_path)\n",
    "\n",
    "\n",
    "print(f\"ingest agreement: {ingest_agreement(file_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5332c79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEASE AGREEMENT - RESIDENTIAL\n",
      "1\n",
      "This is a written contract that sets out the terms and conditions between the Landlord and Tenant of a residential property.\n",
      "(the address acts as the domicilium citandi et executandi)\n",
      "(the address acts as the domicilium citandi et executandi)\n",
      "LEASE AGREEMENT - RESIDENTIAL\n",
      "1\n",
      "This is a written contract that sets out the terms and conditions between the Landlord and Tenant of a residential property.\n",
      "(the address acts as the domicilium citandi et executandi)\n",
      "(the address acts as the domicilium citandi et executandi)\n",
      "DISCLAIMER\n",
      "9\n",
      "LAW\tFOR\tALL\tcares\tabout\tthe\tlegal\trights\tof\tSouth\tAfricans\tand\thave\tmade\tit\tour\tgoal\tto\tmake\tthe\tlaw\taffordable\tand\taccessible\tto\tall.\tThis\tcontract template\thas\tbeen\tdesigned\twith\tyou\tand\tprotection\tof\tyour\trights\tin\tmind.\tAlthough\twe\thave\ttaken\tevery\tcare\tto\tensure\tthat\tthis\tdocument\tis\taccurate\ta\n",
      "DISCLAIMER\n",
      "9\n",
      "LAW\tFOR\tALL\tcares\tabout\tthe\tlegal\trights\tof\tSouth\tAfricans\tand\thave\tmade\tit\tour\tgoal\tto\tmake\tthe\tlaw\taffordable\tand\taccessible\tto\tall.\tThis\tcontract template\thas\tbeen\tdesigned\twith\tyou\tand\tprotection\tof\tyour\trights\tin\tmind.\tAlthough\twe\thave\ttaken\tevery\tcare\tto\tensure\tthat\tthis\tdocument\tis\taccurate\ta\n",
      "13.  ONUS & AGENCY\n",
      "7\n",
      "- 13.1. Where the Tenant must obtain the written permission of the Landlord in terms of this lease agreement, and it is believed that permission is unreasonably refused, the onus of proof lies with the Tenant.\n",
      "- 13.2. The Landlord is entitled to appoint a managing agent to manage this lease on its b\n"
     ]
    }
   ],
   "source": [
    "results = search(\"What is the doc about?\")\n",
    "\n",
    "for r in results:\n",
    "    print(r.metadata[\"section_title\"])\n",
    "    print(r.metadata[\"page_number\"])\n",
    "    print(r.text[:300])\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge-computing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
